<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Mina's Wonderland</title>
 <link href="http://hemina.github.io/atom.xml" rel="self"/>
 <link href="http://hemina.github.io"/>
 <updated>2017-01-12T14:25:37+01:00</updated>
 <id>http://hemina.github.io</id>
 <author>
   <name>Mina He</name>
   <email>mina.he1992@gmail.com</email>
 </author>

 
 <entry>
   <title>Meetup Machine Learning</title>
   <link href="http://hemina.github.io/2017/01/11/meetup-machine-learning"/>
   <updated>2017-01-11T00:00:00+01:00</updated>
   <id>http://hemina.github.io/2017/01/11/meetup-machine-learning</id>
   <content type="html">
&lt;h2 id=&quot;trust-challenge-about-prediction-of-classifier&quot;&gt;Trust challenge about prediction of classifier&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;a href=&quot;https://homes.cs.washington.edu/~marcotcr/&quot;&gt;Marco Tulio Ribeiro&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;Code here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot;&gt;Paper here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To gain trust:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Interpretable models (conclusion + &lt;strong&gt;reason&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;Accuracy&lt;/li&gt;
  &lt;li&gt;A/B Test&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sparse linaire regression&lt;/p&gt;

&lt;p&gt;Mechanical Turker&lt;/p&gt;

&lt;p&gt;LIME:
6 factors to explain the decision&lt;/p&gt;

&lt;h2 id=&quot;apache-samoa&quot;&gt;Apache SAMOA&lt;/h2&gt;
&lt;p&gt;Speaker: 
&lt;a href=&quot;http://albertbifet.com/&quot;&gt;Albert Bifet&lt;/a&gt;, Telecom-Paristech&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/0BzrlDxVZWSUpNFRnb1czZHMyeVVsUFY0SmtrVDIzS1V5NFVn/view&quot;&gt;ppt here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/samoa.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;!--![](/img/samoa.jpg)
--&gt;
&lt;blockquote&gt;
  &lt;p&gt;An open-source platform for mining big data streams with Apache Flink, Storm and Samza. Real time analytics is becoming the fastest and most efficient way to obtain useful knowledge from what is happening now, allowing organizations to react quickly when problems appear or to detect new trends helping to improve their performance.  Apache SAMOA includes algorithms for the most common machine learning tasks such as classification and clustering. It provides a pluggable architecture that allows it to run on Apache Flink, but also with other several distributed stream processing engines such as Storm and Samza.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Real-Time Analytics =&amp;gt; Streaming model&lt;/p&gt;

&lt;p&gt;Need to retrain model with new data&lt;/p&gt;

&lt;p&gt;MOA (Massive Online Analysis)&lt;/p&gt;

&lt;p&gt;Vertical Parallelism&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/01/introduction-online-machine-learning-simplified-2/&quot;&gt;sequential evaluation for online learning&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gan-generative-adversarial-networks&quot;&gt;GAN (Generative Adversarial Networks)&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;a href=&quot;https://www.linkedin.com/in/julien-launay-400a7512a/&quot;&gt;Julien Launay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/uy56cbkf9czww9m/Paris%20Machine%20Learning%20Meetup%20-%20Cracking%20Crack%20Mechanics%20%28NOANIM%29.pdf?dl=0&quot;&gt;ppt here&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When modeling transfers through a medium in civil engineering, knowing the precise influence of cracks is often complicated, doubly so since the transfer and fracture problems are often heavily linked. I will present a new way to generate “fake” cracking patterns using GANs, and will then expand on how such novel techniques can be used to learn more about fracture mechanics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Visualization convolution filters&lt;/p&gt;

&lt;h2 id=&quot;manual-labelling-service&quot;&gt;Manual labelling Service&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/0BzrlDxVZWSUpZXNvV19mRDk4YzB5cmJIWFNCbmZYN3JuVkw4/view&quot;&gt;FouleFactory&lt;/a&gt;
50000 fouleurs (students, retired, enployee)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;http://nuit-blanche.blogspot.fr/2017/01/ce-soir-paris-machine-learning-meetup-5.html&quot;&gt;Here is the official reference blog&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Chinese version reflective writing:&lt;/p&gt;

&lt;p&gt;LIME 项目用来跟受众更好地解释机器学习的决策，适合作为工具在会议中说服公司上层。一个例子，怎样让机器来识别图片中的哈士奇，和狼。LIME通常会给出六大理由，其中理由之一是，图片中有雪时，就判断是哈士奇。&lt;/p&gt;

&lt;p&gt;Apache SAMOA 用来对接实时数据流和各种机器学习算法，实现高效的online learning.&lt;/p&gt;

&lt;p&gt;GAN（Generative Adversarial Networks）
通过产生器和判别器生成可信度高的含label图像训练集，用在计算机视觉领域。&lt;/p&gt;

&lt;p&gt;还有一个来自meetup赞助商的广告，因为内容比较有意思，所以也写进来了。鉴于监督学习在工业界需要大量的人力labelling，这家公司就搭建了一个网络平台，广大闲人们可以通过零散的labelling work赚取外快，业界又可以在短期内迅速获得人力标记的高质量训练集。&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>关于第一篇博文</title>
   <link href="http://hemina.github.io/2017/01/11/"/>
   <updated>2017-01-11T00:00:00+01:00</updated>
   <id>http://hemina.github.io/2017/01/</id>
   <content type="html">
&lt;p&gt;&lt;img src=&quot;/img/notebook.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;昨晚临睡前，给自己的2017心愿单又加了一条，要开启米娜的&lt;code class=&quot;highlighter-rouge&quot;&gt;github.io&lt;/code&gt;博文时代。&lt;/p&gt;

&lt;p&gt;于是今天折腾一整天，心愿初步达成。&lt;/p&gt;

&lt;p&gt;今明两天都有meetup。计划等下把笔记都po到这里来。&lt;/p&gt;

&lt;p&gt;好期待看到这个少女心粉色系的极客日记本慢慢完整的样子。&lt;/p&gt;

</content>
 </entry>
 
 
</feed>
